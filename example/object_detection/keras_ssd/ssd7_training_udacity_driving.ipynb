{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD7 Training Tutorial\n",
    "\n",
    "This tutorial explains how to train an SSD7 on the Udacity road traffic datasets, and just generally how to use this SSD implementation.\n",
    "\n",
    "Disclaimer about SSD7:\n",
    "As you will see below, training SSD7 on the aforementioned datasets yields good results, but I'd like to emphasize that SSD7 is not a carefully optimized network architecture. The idea was just to build a low-complexity network that is fast (roughly 135 FPS or 3 times as fast as SSD300 on a GTX1070). Would slightly different anchor box scaling factors or a slightly different number of filters in individual convolution layers make SSD7 significantly better at similar complexity? I don't know, I haven't tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras_ssd7 import ssd7\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "from ssd_batch_generator import BatchGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set the model configuration parameters\n",
    "\n",
    "The cell below sets a number of parameters that define the model configuration. The parameters set here are being used both by the `ssd7()` function that builds the model as well as further down by the constructor for the `SSDBoxEncoder` object that is needed to run the training.\n",
    "\n",
    "Hence, executing the code cell below is only necessary if you either want create a new model, or if you want to load a previously saved model and train it.\n",
    "\n",
    "If you just want to load a previously saved model and do some inference with it, but no training, you don't need to execute the code cell below.\n",
    "\n",
    "Here are some comments on a few of the parameters, read the documentation for details:\n",
    "\n",
    "* Set the height, width, and number of color channels to whatever you want the model to accept as image input. This does not have to be the actual size of your input images! However, if your input images have a different size than you define as the model input here, you must use the `crop`, `resize` and/or `random_crop` features of the batch generator to convert your images to the model input size during training. If your dataset contains images of varying size, like the Pascal VOC datasets for example, use the `random_crop` feature of the batch generator to cope with that (see the documentation).\n",
    "* The number of classes is the number of positive classes in your dataset, e.g. 20 for Pascal VOC or 80 for MS COCO. Class ID 0 must always be reserved for the background class, i.e. your positive classes must have positive and consecutive integers as their IDs.\n",
    "* The reason why the list of scaling factors has 5 elements even though there are only 4 predictor layers in this model is that the last scaling factor is used for the second aspect-ratio-1 box of the last predictor layer. See the documentation for details.\n",
    "* Alternatively to passing an explicit list of scaling factors, you could also just define a mimimum and a maximum scale, in which case the other scaling factors would be linearly interpolated. If you pass both min/max scaling factors and an explicit list, the explicit list will be used.\n",
    "* `ssd7()` and `SSDBoxEncoder` have two arguments for the anchor box aspect ratios: `aspect_ratios_global` and `aspect_ratios_per_layer`. You can use either of the two. If you use `aspect_ratios_global`, then you just pass a list containing all aspect ratios for which you would like to create anchor boxes. Every aspect ratio you want to include must be listed once and only once. If you use `aspect_ratios_per_layer`, then you pass a list containing lists of aspect ratios for each individual predictor layer. In the example below, the model has four predictor layers, so you would pass a list containing four lists.\n",
    "* If `two_boxes_for_ar1 == True`, then two boxes of different size will be created for aspect ratio 1 for each predictor layer. See the documentation for details.\n",
    "* If `limit_boxes == True`, then the generated anchor boxes will be limited so that they lie entirely within the image boundaries. This feature is called 'clip' in the original Caffe implementation. Even though it may seem counterintuitive, it is recommended **not** to clip the anchor boxes. According to Wei Liu, the model performs slightly better when the anchors are not clipped.\n",
    "* The variances are scaling factors for the target coordinates. Leaving them at 1.0 for each of the four box coordinates means that they have no effect whatsoever. Decreasing them to below 1.0 **upscales** the gradient for the respective target box coordinate.\n",
    "* The `coords` argument lets you choose what coordinate format the model should learn. If you choose the 'centroids' format, the targets will be converted to the (cx, cy, w, h) coordinate format used in the original implementation. If you choose the 'minmax' format, the targets will be converted to the coordinate format (xmin, xmax, ymin, ymax). The model, of course, will learn whatever the targets tell it to.\n",
    "* `normalize_coords` converts all absolute ground truth and anchor box coordinates to relative coordinates, i.e. to coordinates that lie within [0,1] relative to the image height and width.\n",
    "\n",
    "These paramters might be a bit much at first, but they allow you to configure many things easily.\n",
    "\n",
    "The parameters set below are not only needed to build the model, but are also passed to the `SSDBoxEncoder` constructor in the subsequent cell, which is responsible for matching and encoding ground truth boxes and anchor boxes during training. In order to do that, it needs to know the anchor box specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the input images\n",
    "img_width = 480 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "subtract_mean = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "divide_by_stddev = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "n_classes = 5 # Number of positive classes\n",
    "scales = [0.08, 0.16, 0.32, 0.64, 0.96] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "aspect_ratios = [0.5, 1.0, 2.0] # The list of aspect ratios for the anchor boxes\n",
    "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "limit_boxes = False # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
    "coords = 'centroids' # Whether the box coordinates to be used should be in the 'centroids' or 'minmax' format, see documentation\n",
    "normalize_coords = True # Whether or not the model is supposed to use relative coordinates that are within [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build or load the model\n",
    "\n",
    "You will want to execute either of the two code cells in the subsequent two sub-sections, not both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a new model\n",
    "\n",
    "If you want to create a new model, this is the relevant section for you. If you want to load a previously saved model, skip ahead to section 2.2.\n",
    "\n",
    "The code cell below does the following things:\n",
    "1. It calls the function `ssd7()` to build the model.\n",
    "2. It then compiles the model for the training. In order to do so, we're defining an optimizer (Adam) and a loss function (SSDLoss) to be passed to the `compile()` method.\n",
    "\n",
    "`SSDLoss` is a custom Keras loss function that implements the multi-task log loss for classification and smooth L1 loss for localization. `neg_pos_ratio` and `alpha` are set as in the paper and `n_neg_min` is a rather unimportant optional parameter to make sure that a certain number of negative boxes always enters the loss function even if there are very few or no positive boxes in a batch, which should never happen anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd7(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    limit_boxes=limit_boxes,\n",
    "                    variances=variances,\n",
    "                    coords=coords,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=subtract_mean,\n",
    "                    divide_by_stddev=divide_by_stddev,\n",
    "                    swap_channels=False)\n",
    "\n",
    "# 2: Optional: Load some weights\n",
    "\n",
    "#model.load_weights('model/ssd7_udacity_driving_weights.h5')\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "sgd = SGD(lr=5e-3, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load a saved model\n",
    "\n",
    "If you have previously created and saved a model and would now like to load it, simply execute the next code cell. The only thing you need to do is to set the path to the saved model HDF5 file that you would like to load.\n",
    "\n",
    "The SSD model contains custom objects: Neither the loss function, nor the anchor box or L2-normalization layer types are contained in the Keras core library, so we need to provide them to the model loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "model_path = 'model/ssd7_udacity_driving.h5'\n",
    "\n",
    "# We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "                                               'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up the data generators for the training\n",
    "\n",
    "The code cells below set up batch generators for the training and validation datasets to train the model. You will have to set are the correct file paths to the images and labels of your dataset, and in case your labels do not come in a CSV file, you might have to switch from the CSV parser to the XML parser or you might have to write a new parser method in the `BatchGenerator` class that can handle whatever the format of your labels is. The [README](https://github.com/pierluigiferrari/ssd_keras/blob/master/README.md) of this project provides an overview of the design of the batch generator class, which should help you in case you need to write a new parser or adapt one of the existing parsers to your needs.\n",
    "\n",
    "Set the batch size to whatever value you like (and that your GPU memory allows), it's not the most important hyperparameter - 32 works well, but so do most other batch sizes.\n",
    "\n",
    "The `ssd_box_encoder` object, which, as explained above, knows how to match and encode the ground truth labels into the format that the model needs, is passed to the batch generator, which during training loads the next batch of images and labels, optionally performs data augmentation, and encodes the ground truth labels.\n",
    "\n",
    "There are two parameters in the SSDBoxEncoder that you should note: `pos_iou_threshold` and `neg_iou_threshold`. The former determines the minimum Jaccard overlap between a ground truth box and an anchor box for a match and is set to 0.5, the value stated in the paper. The latter, `neg_iou_threshold`, is not in the paper, but it is useful to improve the learning process. It determines the maximum allowed Jaccard overlap between an anchor box and any ground truth box in order for that anchor box to be considered a negative box. This is useful because you want a clear margin between negative and positive boxes. An anchor box that almost contains an object should not be forced to learn to predict a negative box in such a case. 0.2 is a reasonable value that is used by various other object detection models.\n",
    "\n",
    "Here is a brief overview of how the ground truth encoding and model output decoding are designed. The ground truth box matching and encoding happens as part of the mini batch generation. To be specific, the `generate()` method of `BatchGenerator` calls the `encode_y()` method of `SSDBoxEncoder` to encode the ground truth labels, and then yields the matched and encoded target tensor to be passed to the loss function. The decoding of the raw model output, confidence thresholding, and non-maximum suppression (NMS) is then performed by `decode_y()` and `decode_y2()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "The example setup below was used to train SSD7 on two road traffic datasets released by [Udacity](https://github.com/udacity/self-driving-car/tree/master/annotations) with around 20,000 images in total and 5 object classes (car, truck, pedestrian, bicyclist, traffic light), although the vast majority of the objects are cars. The original datasets have a constant image size of 1200x1920 RGB. I consolidated the two datasets, removed a few bad samples (although there are probably many more), and resized the images to 300x480 RGB, i.e. to one sixteenth of the original image size. In case you'd like to train a model on the same dataset, you can download the consolidated and resized dataset I used [here](https://drive.google.com/open?id=1uOqIUiJlDwoeL8vnNMacNbkDpDe1eRp-) (about 900 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Instantiate to `BatchGenerator` objects: One for training, one for validation.\n",
    "\n",
    "train_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "val_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "# TODO: Set the paths to your dataset here.\n",
    "\n",
    "# Images\n",
    "images_dir = '/datasets/udacity_driving_datasets/'\n",
    "\n",
    "# Ground truth\n",
    "train_labels_filename = '/datasets/udacity_driving_datasets/labels_train.csv'\n",
    "val_labels_filename   = '/datasets/udacity_driving_datasets/labels_val.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "# Get the number of samples in the training and validations datasets to compute the epoch lengths below.\n",
    "n_train_samples = train_dataset.get_n_samples()\n",
    "n_val_samples = val_dataset.get_n_samples()\n",
    "\n",
    "print(\"n_train:\", n_train_samples)\n",
    "print(\"n_val:\", n_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "In the next cell we'll set some data augmentation options for the batch generator. These settings work, but they are not meant to be the ideal settings. Maybe different settings (for example more extreme augmentation) would yield better results. Experiment with this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function. \n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
    "                                img_width=img_width,\n",
    "                                n_classes=n_classes, \n",
    "                                predictor_sizes=predictor_sizes,\n",
    "                                scales=scales,\n",
    "                                aspect_ratios_global=aspect_ratios,\n",
    "                                aspect_ratios_per_layer=None,\n",
    "                                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                steps=steps,\n",
    "                                offsets=offsets,\n",
    "                                limit_boxes=limit_boxes,\n",
    "                                variances=variances,\n",
    "                                pos_iou_threshold=0.5,\n",
    "                                neg_iou_threshold=0.2,\n",
    "                                coords=coords,\n",
    "                                normalize_coords=normalize_coords)\n",
    "\n",
    "# 4: Set the batch size.\n",
    "\n",
    "batch_size = 32 # Change the batch size if you like, or if you run into memory issues with your GPU.\n",
    "\n",
    "# 5: Set the image processing / data augmentation options and create generator handles.\n",
    "\n",
    "# Change the ad-hoc data augmentation settings as you like\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         train=True,\n",
    "                                         ssd_box_encoder=ssd_box_encoder,\n",
    "                                         convert_to_3_channels=True,\n",
    "                                         equalize=False,\n",
    "                                         brightness=(0.5, 2, 0.5), # Randomly change brightness between 0.5 and 2 with probability 0.5\n",
    "                                         flip=0.5, # Randomly flip horizontally with probability 0.5\n",
    "                                         translate=((5, 70), (3, 50), 0.5), # Randomly translate by 5-70 pixels horizontally and 3-50 pixels vertically with probability 0.5\n",
    "                                         scale=(0.7, 1.4, 0.5), # Randomly scale between 0.7 and 1.4 with probability 0.5\n",
    "                                         max_crop_and_resize=False,\n",
    "                                         random_pad_and_resize=False,\n",
    "                                         random_crop=False,\n",
    "                                         crop=False,\n",
    "                                         resize=False,\n",
    "                                         gray=False,\n",
    "                                         limit_boxes=True,\n",
    "                                         include_thresh=0.4)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     train=True,\n",
    "                                     ssd_box_encoder=ssd_box_encoder,\n",
    "                                     convert_to_3_channels=True,\n",
    "                                     equalize=False,\n",
    "                                     brightness=False,\n",
    "                                     flip=False,\n",
    "                                     translate=False,\n",
    "                                     scale=False,\n",
    "                                     max_crop_and_resize=False,\n",
    "                                     random_pad_and_resize=False,\n",
    "                                     random_crop=False,\n",
    "                                     crop=False,\n",
    "                                     resize=False,\n",
    "                                     gray=False,\n",
    "                                     limit_boxes=True,\n",
    "                                     include_thresh=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the training\n",
    "\n",
    "Now that everything is set up, we're ready to start training. Set the number of epochs and the model name, the weights name in `ModelCheckpoint` and the filepaths to wherever you'd like to save the model. There isn't much more to say here, just execute the cell. If you get \"out of memory\" errors during training, reduce the batch size.\n",
    "\n",
    "Note that the number of epochs is arbitrarily set to 30 here. This does not imply that training for 30 epochs will be enough. You will usually need to train your model for tens of thousands of training steps. The number of training steps that one epoch consists of depends on the size of your training dataset and on the chosen batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 setup callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(log_dir='/log/tensorboard', batch_size=batch_size)\n",
    "model_check_point = ModelCheckpoint('model/ssd7_udacity_driving_epoch-{epoch:02d}_loss-{loss:.4f}_valloss-{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=False,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.001, patience=10)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='loss',\n",
    "                                         factor=0.1,\n",
    "                                         patience=5,\n",
    "                                         epsilon=0.001,\n",
    "                                         min_lr=1e-6,\n",
    "                                         cooldown=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Set the number of epochs to train for.\n",
    "epochs = 200\n",
    "\n",
    "history = model.fit_generator(generator = train_generator,\n",
    "                              steps_per_epoch = ceil(n_train_samples/batch_size),\n",
    "                              epochs = epochs,\n",
    "                              callbacks = [model_check_point,\n",
    "                                           early_stopping,\n",
    "                                           reduce_lr_on_plateau,\n",
    "                                           tensor_board],\n",
    "                              validation_data = val_generator,\n",
    "                              validation_steps = ceil(n_val_samples/batch_size))\n",
    "\n",
    "# TODO: Set the filename (without the .h5 file extension!) under which to save the model and weights.\n",
    "#       Do the same in the `ModelCheckpoint` callback above.\n",
    "model_name = 'model/ssd7_udacity_driving'\n",
    "model.save('{}.h5'.format(model_name))\n",
    "model.save_weights('{}_weights.h5'.format(model_name))\n",
    "\n",
    "print()\n",
    "print(\"Model saved under {}.h5\".format(model_name))\n",
    "print(\"Weights also saved separately under {}_weights.h5\".format(model_name))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the training and validation loss evolved to check whether our training is going in the right direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right', prop={'size': 24});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, the validation loss has been decreasing at a similar pace as the training loss, indicating that our model has been learning effectively over the last 30 epochs. We could try to train longer and see if the validation loss can be decreased any further. Once the validation loss stops decreasing for a couple of epochs in a row, that's when we will want to stop training. Our final weights will then be the weights of the epoch that had the lowest validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make predictions\n",
    "\n",
    "Now let's make some predictions on the validation dataset with the trained model. For convenience we'll use the validation generator which we've already set up above. Feel free to change the batch size.\n",
    "\n",
    "You can set the `shuffle` option to `False` if you would like to check the model's progress on the same image(s) over the course of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make predictions\n",
    "\n",
    "# 1: Set the generator\n",
    "\n",
    "predict_generator = val_dataset.generate(batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         train=False,\n",
    "                                         returns={'processed_labels',\n",
    "                                                  'filenames'},\n",
    "                                         convert_to_3_channels=True,\n",
    "                                         equalize=False,\n",
    "                                         brightness=False,\n",
    "                                         flip=False,\n",
    "                                         translate=False,\n",
    "                                         scale=False,\n",
    "                                         max_crop_and_resize=False,\n",
    "                                         random_pad_and_resize=False,\n",
    "                                         random_crop=False,\n",
    "                                         crop=False,\n",
    "                                         resize=False,\n",
    "                                         gray=False,\n",
    "                                         limit_boxes=True,\n",
    "                                         include_thresh=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Generate samples\n",
    "\n",
    "X, y_true, filenames = next(predict_generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(y_true[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Make a prediction\n",
    "\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's decode the raw prediction `y_pred`. The function `decode_y2()` converts the box coordinates from offsets back to absolute coordinates, keeps only the positive predictions (i.e. throws away all boxes for which the highest confidence is for class 0), applies a confidence threshold to all positive predictions, and applies non-maximum suppression to the remaining predictions, in this order. In case you would like to omit the NMS step, set `iou_threshold = None`.\n",
    "\n",
    "You could also use `decode_y()`, which follows the prodecure outlined in the paper, to decode the raw predictions. The main point in which `decode_y()` and `decode_y2()` differ is that `decode_y2()` performs NMS globally and `decode_y()` performs NMS per class. It is important to understand what difference that makes. One point is that doing NMS per class for 20 classes will take roughly 20-times the time to do NMS just once over all classes, but this slow-down doesn't matter much when decoding a single batch. The more important point is to understand what difference it can make for the resulting final predictions. Performing NMS globally means that the strongest candidate box will eliminate all close boxes around it regardless of their predicted class. This can be good and bad. For example, if one box correctly predicts a sheep and another box incorrectly predicts a cow at similar coordinates, then global NMS would eliminate the incorrect cow box (because it is too close to the correct sheep box), while per-class NMS would not eliminate the incorrect cow box (because boxes are only compared within the same object class). On the other hand, if two objects of different classes are very close together and overlapping and are predicted correctly, then global NMS might eliminate one of the two correct predictions because they are too close together, while per-class NMS will keep both predictions. It's up to you which decoder you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode the raw prediction `y_pred`\n",
    "\n",
    "y_pred_decoded = decode_y2(y_pred,\n",
    "                           confidence_thresh=0.5,\n",
    "                           iou_threshold=0.4,\n",
    "                           top_k='all',\n",
    "                           input_coords='centroids',\n",
    "                           normalize_coords=True,\n",
    "                           img_height=img_height,\n",
    "                           img_width=img_width)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Decoded predictions (output format is [class_id, confidence, xmin, ymin, xmax, ymax]):\\n\")\n",
    "print(y_pred_decoded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's draw the predicted boxes onto the image in blue to visualize the result. Each predicted box says its confidence next to the category name. The ground truth boxes are also drawn onto the image in green for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(X[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 6)).tolist() # Set the colors for the bounding boxes\n",
    "classes = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light'] # Just so we can print class names onto the image instead of IDs\n",
    "\n",
    "# Draw the ground truth boxes in green (omit the label for more clarity)\n",
    "for box in y_true[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    #current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "# Draw the predicted boxes in blue\n",
    "for box in y_pred_decoded[i]:\n",
    "    xmin = box[-4]\n",
    "    ymin = box[-3]\n",
    "    xmax = box[-2]\n",
    "    ymax = box[-1]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
