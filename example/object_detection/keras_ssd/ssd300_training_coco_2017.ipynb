{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Training on COCO 2017\n",
    "## Reference: https://github.com/pierluigiferrari/ssd_keras/blob/master/ssd300_training.ipynb\n",
    "- Evaluate a trained SSD300 on one of the MS COCO datasets using the official MS COCO Python tools available here: https://github.com/cocodataset/cocoapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "# TODO: Specify the directory that contains the `pycocotools` here.\n",
    "pycocotools_dir = '/datasets/coco_2017/cocoapi-master/PythonAPI/'\n",
    "if pycocotools_dir not in sys.path:\n",
    "    sys.path.insert(0, pycocotools_dir)\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from keras_ssd300 import ssd_300\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layer_L2Normalization import L2Normalization\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "from ssd_batch_generator import BatchGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set the model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the input images\n",
    "img_width = 300 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "subtract_mean = [123, 117, 104] # The per-channel mean of the images in the dataset\n",
    "swap_channels = True # The color channel order in the original SSD is BGR\n",
    "n_classes = 80 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_coco\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "limit_boxes = False # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
    "coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids', 'corners', or 'minmax' format, see documentation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build or load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create a new model and load trained VGG-16 weights into it (or trained SSD weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                limit_boxes=limit_boxes,\n",
    "                variances=variances,\n",
    "                coords=coords,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=subtract_mean,\n",
    "                divide_by_stddev=None,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# 2: Load the trained VGG-16 weights into the model.\n",
    "\n",
    "# TODO: Set the path to the VGG-16 weights.\n",
    "weights_path = 'model/vgg-16_ssd-fcn_ILSVRC-CLS-LOC.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-04)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load a previously created model (Option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "# model_path = 'model/ssd300_coco.h5'\n",
    "\n",
    "# We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "# ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "# K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "# model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "#                                                'L2Normalization': L2Normalization,\n",
    "#                                                'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set up the data generators for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Instantiate to `BatchGenerator` objects: One for training, one for validation.\n",
    "\n",
    "train_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "val_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "# TODO: Set the paths to the dataset here.\n",
    "MS_COCO_train_images_dir = '/datasets/coco_2017/train2017/'\n",
    "MS_COCO_val_images_dir = '/datasets/coco_2017/val2017/'\n",
    "MS_COCO_train_annotations_filename = '/datasets/coco_2017/annotations/instances_train2017.json'\n",
    "MS_COCO_val_annotations_filename = '/datasets/coco_2017/annotations/instances_val2017.json'\n",
    "\n",
    "train_dataset.parse_json(images_dirs=[MS_COCO_train_images_dir],\n",
    "                   annotations_filenames=[MS_COCO_train_annotations_filename],\n",
    "                   ground_truth_available=True,\n",
    "                   include_classes='all',\n",
    "                   ret=False)\n",
    "\n",
    "val_dataset.parse_json(images_dirs=[MS_COCO_val_images_dir],\n",
    "                   annotations_filenames=[MS_COCO_val_annotations_filename],\n",
    "                   ground_truth_available=True,\n",
    "                   include_classes='all',\n",
    "                   ret=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
    "                                img_width=img_width,\n",
    "                                n_classes=n_classes,\n",
    "                                predictor_sizes=predictor_sizes,\n",
    "                                min_scale=None,\n",
    "                                max_scale=None,\n",
    "                                scales=scales,\n",
    "                                aspect_ratios_global=None,\n",
    "                                aspect_ratios_per_layer=aspect_ratios,\n",
    "                                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                steps=steps,\n",
    "                                offsets=offsets,\n",
    "                                limit_boxes=limit_boxes,\n",
    "                                variances=variances,\n",
    "                                pos_iou_threshold=0.5,\n",
    "                                neg_iou_threshold=0.2,\n",
    "                                coords=coords,\n",
    "                                normalize_coords=normalize_coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Set the batch size.\n",
    "\n",
    "batch_size = 32 # Change the batch size if you like, or if you run into memory issues with your GPU.\n",
    "\n",
    "# 5: Set the image processing / data augmentation options and create generator handles.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         train=True,\n",
    "                                         ssd_box_encoder=ssd_box_encoder,\n",
    "                                         convert_to_3_channels=True,\n",
    "                                         equalize=False,\n",
    "                                         brightness=(0.5, 2, 0.5),\n",
    "                                         flip=0.5,\n",
    "                                         translate=False,\n",
    "                                         scale=False,\n",
    "                                         max_crop_and_resize=(img_height, img_width, 1, 3), # This one is important because the Pascal VOC images vary in size\n",
    "                                         random_pad_and_resize=(img_height, img_width, 1, 3, 0.5), # This one is important because the Pascal VOC images vary in size\n",
    "                                         random_crop=False,\n",
    "                                         crop=False,\n",
    "                                         resize=False,\n",
    "                                         gray=False,\n",
    "                                         limit_boxes=True, # While the anchor boxes are not being clipped, the ground truth boxes should be\n",
    "                                         include_thresh=0.4)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     train=True,\n",
    "                                     ssd_box_encoder=ssd_box_encoder,\n",
    "                                     convert_to_3_channels=True,\n",
    "                                     equalize=False,\n",
    "                                     brightness=False,\n",
    "                                     flip=False,\n",
    "                                     translate=False,\n",
    "                                     scale=False,\n",
    "                                     max_crop_and_resize=(img_height, img_width, 1, 3), # This one is important because the Pascal VOC images vary in size\n",
    "                                     random_pad_and_resize=(img_height, img_width, 1, 3, 0.5), # This one is important because the Pascal VOC images vary in size\n",
    "                                     random_crop=False,\n",
    "                                     crop=False,\n",
    "                                     resize=False,\n",
    "                                     gray=False,\n",
    "                                     limit_boxes=True,\n",
    "                                     include_thresh=0.4)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets to compute the epoch lengths below.\n",
    "n_train_samples = train_dataset.get_n_samples()\n",
    "n_val_samples   = val_dataset.get_n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule.\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch <= 100: return 0.001\n",
    "    else: return 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Set the training callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve on tensor board\n",
    "tensor_board = TensorBoard(log_dir='/log/tensorboard', batch_size=batch_size)\n",
    "\n",
    "model_checkpoint=ModelCheckpoint('model/ssd300_coco_2017_weights_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                                 monitor='val_loss', verbose=1,save_best_only=True, \n",
    "                                                 save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Run Gradiaent Decent and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the number of epochs to train for.\n",
    "epochs = 60\n",
    "\n",
    "history = model.fit_generator(generator = train_generator,\n",
    "                              steps_per_epoch = ceil(n_train_samples/batch_size),\n",
    "                              epochs = epochs,\n",
    "                              callbacks = [model_checkpoint, learning_rate_scheduler, early_stopping, tensor_board],\n",
    "                              validation_data = val_generator,\n",
    "                              validation_steps = ceil(n_val_samples/batch_size))\n",
    "\n",
    "# TODO: Set the filename (without the .h5 file extension!) under which to save the model and weights.\n",
    "#       Do the same in the `ModelCheckpoint` callback above.\n",
    "model_name = 'model/ssd300_coco_2017'\n",
    "model.save('{}.h5'.format(model_name))\n",
    "model.save_weights('{}_weights.h5'.format(model_name))\n",
    "\n",
    "print(\"Model saved under {}.h5\".format(model_name))\n",
    "print(\"Weights also saved separately under {}_weights.h5\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Set the generator\n",
    "\n",
    "predict_generator = val_dataset.generate(batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         train=False,\n",
    "                                         returns={'processed_labels',\n",
    "                                                  'filenames'},\n",
    "                                         convert_to_3_channels=True,\n",
    "                                         equalize=False,\n",
    "                                         brightness=False,\n",
    "                                         flip=False,\n",
    "                                         translate=False,\n",
    "                                         scale=False,\n",
    "                                         max_crop_and_resize=(300, 300, 1, 3),\n",
    "                                         random_pad_and_resize=(300, 300, 1, 3, 0.5),\n",
    "                                         random_crop=False,\n",
    "                                         crop=False,\n",
    "                                         resize=False,\n",
    "                                         gray=False,\n",
    "                                         limit_boxes=True,\n",
    "                                         include_thresh=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Generate samples\n",
    "\n",
    "X, y_true, filenames = next(predict_generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(y_true[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Make a prediction\n",
    "\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Decode the raw prediction `y_pred`\n",
    "\n",
    "y_pred_decoded = decode_y(y_pred,\n",
    "                          confidence_thresh=0.5,\n",
    "                          iou_threshold=0.4,\n",
    "                          top_k=200,\n",
    "                          input_coords='centroids',\n",
    "                          normalize_coords=normalize_coords,\n",
    "                          img_height=img_height,\n",
    "                          img_width=img_width)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('    class    conf  xmin    ymin    xmax    ymax')\n",
    "print(y_pred_decoded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(X[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in y_true[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "for box in y_pred_decoded[i]:\n",
    "    xmin = box[-4]\n",
    "    ymin = box[-3]\n",
    "    xmax = box[-2]\n",
    "    ymax = box[-1]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
